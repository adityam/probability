[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Outline",
    "section": "",
    "text": "Instructor\n\n\nAditya Mahajan\nOffice Hours: TBD\n\n\nTeaching Assistants\n\n\nAlexander Fernandes\n\n\nLectures\n\n\n2:35pm–3:55pm Monday, Wednesday (ENGTR 2100)\n\n\nTutorials\n\n\n12:35am–1:25am Friday, (ENGTR 2100)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nECSE 206 or ECSE 316 (Signals and Systems)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nMaterial Covered\n\n\n\n\n1\nProbability spaces, algebra of events, axioms of probability\n\n\n2\nRandom variables and random vectors\n\n\n3\nDiscrete and continuous random variables\n\n\n4\nConditional distributions and conditional expectations\n\n\n5\nMoment generating functions and sums of random variables\n\n\n6\nProbability inequalities\n\n\n7\nReview and Mid-Term\n\n\n8\nConvergence in probability and the weak law of large numbers\n\n\n9\nAlmost sure convergence and the strong law of large numbers\n\n\n10\nMaximum likelihood estimation and minimum mean squared error estimation\n\n\n11\nStochastic processes, Bernoulli process, Poisson process, and Gaussian process\n\n\n12\nMarkov chains\n\n\n13\nWide sense stationary processes\n\n\n\n\n\n\n\nTextbook\n\n\nGrimmett and Stirzaker, “Probability and Random Processes”, 4th edition, Oxford University Press\n\n\n\n\n\n\n\nAssignments (25%) Weekly homework assignments. Typically, each assignment will consist of four questions, out of which one or two randomly selected questions will be grader. The lowest two homework assignments will be dropped.\nMid Term (25%) Closed book in-class exam. Oct 9 (during class time)\nFinal Exam (50%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term.\n\n\n\n\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\nDue to paucity of grading hours, only one or two randomly selected questions will be graded in each assignment.\nThe lowest two assignments and labs will be dropped. There will be no make-up for missed assignments and labs, even if it is for a valid reason. The whole point of dropping the lowest two assignments/labs is to reduce the administrative overhead of keeping track of such missed assignments/labs.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)\n\n\n\n\n\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.\n\n\n\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students."
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Course Outline",
    "section": "",
    "text": "Instructor\n\n\nAditya Mahajan\nOffice Hours: TBD\n\n\nTeaching Assistants\n\n\nAlexander Fernandes\n\n\nLectures\n\n\n2:35pm–3:55pm Monday, Wednesday (ENGTR 2100)\n\n\nTutorials\n\n\n12:35am–1:25am Friday, (ENGTR 2100)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nECSE 206 or ECSE 316 (Signals and Systems)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Course Outline",
    "section": "",
    "text": "Week\nMaterial Covered\n\n\n\n\n1\nProbability spaces, algebra of events, axioms of probability\n\n\n2\nRandom variables and random vectors\n\n\n3\nDiscrete and continuous random variables\n\n\n4\nConditional distributions and conditional expectations\n\n\n5\nMoment generating functions and sums of random variables\n\n\n6\nProbability inequalities\n\n\n7\nReview and Mid-Term\n\n\n8\nConvergence in probability and the weak law of large numbers\n\n\n9\nAlmost sure convergence and the strong law of large numbers\n\n\n10\nMaximum likelihood estimation and minimum mean squared error estimation\n\n\n11\nStochastic processes, Bernoulli process, Poisson process, and Gaussian process\n\n\n12\nMarkov chains\n\n\n13\nWide sense stationary processes"
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Course Outline",
    "section": "",
    "text": "Textbook\n\n\nGrimmett and Stirzaker, “Probability and Random Processes”, 4th edition, Oxford University Press"
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Course Outline",
    "section": "",
    "text": "Assignments (25%) Weekly homework assignments. Typically, each assignment will consist of four questions, out of which one or two randomly selected questions will be grader. The lowest two homework assignments will be dropped.\nMid Term (25%) Closed book in-class exam. Oct 9 (during class time)\nFinal Exam (50%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term."
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Course Outline",
    "section": "",
    "text": "Assignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\nDue to paucity of grading hours, only one or two randomly selected questions will be graded in each assignment.\nThe lowest two assignments and labs will be dropped. There will be no make-up for missed assignments and labs, even if it is for a valid reason. The whole point of dropping the lowest two assignments/labs is to reduce the administrative overhead of keeping track of such missed assignments/labs.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)"
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Course Outline",
    "section": "",
    "text": "The course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures."
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Course Outline",
    "section": "",
    "text": "As the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students."
  },
  {
    "objectID": "probability-spaces.html",
    "href": "probability-spaces.html",
    "title": "Introduction to Probability",
    "section": "",
    "text": "This is a graduate course on probability and random signals. I am going to assume that everyone is familiar with the basics of undergraduate probability. For example, you should be able to answer the following questions:\n\nA fair 6-sided die is rolled twice. What is the probability that the sum of the rolls equals 7?\nA biased coin with \\(\\PR({\\rm heads}) = 3/4\\) is tossed 10 times. What is the probability of obtaining 3 consecutive heads?\n\nYou should also be familiar with the following concepts:\n\nRandom variables, probability distributions, and expectations\nConditional distributions, independent random variables\n\nSome of you might also have seen the following concepts\n\nLaw of large numbers\nCentral limit theorem\n\nIn this course, we revisit these topics with a more formal approach. We start with a review of the basic concepts."
  },
  {
    "objectID": "probability-spaces.html#background",
    "href": "probability-spaces.html#background",
    "title": "Introduction to Probability",
    "section": "",
    "text": "This is a graduate course on probability and random signals. I am going to assume that everyone is familiar with the basics of undergraduate probability. For example, you should be able to answer the following questions:\n\nA fair 6-sided die is rolled twice. What is the probability that the sum of the rolls equals 7?\nA biased coin with \\(\\PR({\\rm heads}) = 3/4\\) is tossed 10 times. What is the probability of obtaining 3 consecutive heads?\n\nYou should also be familiar with the following concepts:\n\nRandom variables, probability distributions, and expectations\nConditional distributions, independent random variables\n\nSome of you might also have seen the following concepts\n\nLaw of large numbers\nCentral limit theorem\n\nIn this course, we revisit these topics with a more formal approach. We start with a review of the basic concepts."
  },
  {
    "objectID": "probability-spaces.html#review-of-set-theory",
    "href": "probability-spaces.html#review-of-set-theory",
    "title": "Introduction to Probability",
    "section": "2 Review of Set Theory",
    "text": "2 Review of Set Theory\n\n2.1 Basic set operations\nA set is a collection of objects. We say that a set \\(B\\) is a subset of set \\(A\\) (written as \\(B \\subseteq A\\)) if all elements of \\(B\\) are also elements of \\(A\\). We say that \\(B\\) is a proper subset (written \\(B \\subsetneq A\\)) if \\(B \\subseteq A\\) and \\(B \\neq A\\).\n\nExercise 1 Let \\(A = \\{1, 2, 3\\}\\). Find all subsets of \\(A\\).\n\nThe set of all subsets of \\(A\\) is also called the power set of \\(A\\) (denoted by \\(2^A\\)). The notation \\(2^A\\) is capturing the fact that the power set of \\(A\\) has \\(2^{|A|}\\) elements. For example, your answer to Exercise 1 must have \\(2^3 = 8\\) elements.\nGiven two sets \\(A\\) and \\(B\\), we define the set difference \\(A\\setminus B\\) to be all elements of \\(A\\) not in \\(B\\). Note that mathematically \\(A \\setminus B\\) is well defined even when \\(B \\not\\subseteq A\\). In particular \\[\nA \\setminus B = A \\setminus (A \\cap B).\n\\]\n\nExercise 2 Compute \\(A \\setminus B\\) for the following:\n\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2\\}\\).\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2, 5\\}\\).\n\n\nGiven a collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) of sets, we define two operations:\n\nUnion \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) as follows \\[\n  \\bigcup_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for some } i \\}\n\\] i.e., an element belongs to \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) if it belongs to at least one of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\nIntersection \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) as follows \\[\n  \\bigcap_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for all } i \\}\n\\] i.e., an element belongs to \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) if it belongs to all of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\n\nA collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) is disjoint if for every \\(i \\neq j\\), \\(A_i \\cap A_j = \\emptyset\\), where \\(\\emptyset\\) denotes the empty set.\nGiven a universal set \\(Ω\\) and a collection \\(\\{B_1, B_2, \\dots, B_m\\}\\) of subsets of \\(Ω\\), we say that \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) if \\(\\{B_1, B_2, \\dots, B_m\\}\\) are disjoint and \\(\\bigcup_{i=1}^m B_i = Ω\\).\n\n\n\n\n\n\n\n\nFigure 1: Example of a partition\n\n\n\n\nExample 1 Let \\(Ω = \\{1,2,3,4\\}\\). The following are partitions of \\(Ω\\):\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\}\\).\n\\(\\{ \\{1, 2\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4\\} \\}\\).\n\nThe follow are not partitions of \\(Ω\\) [Explain why?]\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\}\\).\n\\(\\{ \\{1, 2, 3\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4, 5\\} \\}\\).\n\n\nIn most of our discussion, we will work with a pre-specified universal set \\(Ω\\). In this setting we use \\(A^c\\) (read: \\(A\\)-complement) as a short hand for \\(Ω\\setminus A\\).\n\n\n\n\n\n\nPartitions are useful because they allow breaking up a set into disjoint pieces. In particular, suppose \\(\\{B_1, \\dots, B_m\\}\\) is a partition and \\(A\\) is any subset of \\(Ω\\).\nThen, \\[\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_m)\n\\] where each of the components are disjoint.\n\n\n\n\n\n2.2 Properties of set operations\n\nCommutative \\[A \\cup B = B \\cup A\n\\quad\\text{and}\\quad\nA \\cap B = B \\cap A\\]\nAssociative \\[A \\cup (B \\cup C)= (A \\cup B) \\cup C\n\\quad\\text{and}\\quad\nA \\cap (B \\cap C)= (A \\cap B) \\cap C\\]\nDistributive \\[A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cap C)\n\\quad\\text{and}\\quad\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\]\nDe Morgan’s Law \\[(A \\cup B)^c = A^c \\cap B^\\cap\n\\quad\\text{and}\\quad\n(A \\cap B)^c = A^c \\cup B^c\\]\n\n\nExercise 3 Use distributive property to simplify:\n\n\\([1,4] \\cap ([0,2] \\cup [3,5])\\).\n\\([2,4] \\cup ([3,5] \\cap [1,4])\\).\n\n\n\n\n2.3 An algebra (or field) on sets\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots, F_m\\}\\) of subsets of \\(Ω\\) is called an algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under finite unions and finite intersections: if \\(A_1, \\dots, A_n \\in \\ALPHABET F\\), then \\[\nA_1 \\cup A_2 \\cup \\cdots \\cup A_n \\in \\ALPHABET F\n\\quad\\text{and}\\quad\nA_1 \\cap A_2 \\cap \\cdots \\cap A_n \\in \\ALPHABET F\n\\]\n\nWe will sometimes use the notation “\\((Ω,\\ALPHABET F)\\) is an algebra of sets” or “\\(\\ALPHABET F\\) is an algebra on \\(Ω\\)”. Some examples of algebras are as follows:\n\nThe smallest algebra associated with \\(Ω\\) is \\(\\{\\emptyset, Ω\\}\\).\nIf \\(A\\) is any subset of \\(Ω\\), then \\(\\{\\emptyset, A, A^c, Ω\\}\\) is an algebra.\nFor any set \\(Ω\\), the power-set \\(2^Ω\\) is an algebra on \\(Ω\\). As an illustration, check that the power set defined in Exercise 1 is an algebra.\n\nThese are all examples of a general principle: The power-set of any partition of a set is an algebra.\n\nIf the partition is \\(\\{Ω\\}\\), then the power-set \\(\\{ \\emptyset, Ω \\}\\) is an algebra.\nIf the partition is \\(\\{A, A^c\\}\\), then the power-set \\(\\{ \\emptyset, A, A^c, Ω\\}\\) is an algebra.\nIf the partition is the collection of all singleton elements of a set, then the power-set \\(2^Ω\\) is an algebra."
  },
  {
    "objectID": "probability-spaces.html#probability-space",
    "href": "probability-spaces.html#probability-space",
    "title": "Introduction to Probability",
    "section": "3 Probability Space",
    "text": "3 Probability Space\nProbability is a measure of uncertainty or our belief that a particular statement is true. In this course, we will not concerns ourselves with how such a measure of uncertainty is constructed; rather focus on the mathematical properties that it should satisfy and the implications of these properties.\nMany everyday statements take the form: “The chance (or probability) of \\(A\\) is \\(p\\)”, where \\(A\\) is some event (such as “sun shining tomorrow”, “Team A winning a hockey game”, etc.) and \\(p\\) is a number (e.g., \\(1/8\\)) or an adjective describing quantity (e.g., “low”).\nTo mathematically model such statements, we need to model the sequence of events that may lead to the occurrence of \\(A\\): this is called a random experiment; the result of an experiment is called an outcome.\nIn general, the outcome of an experiment is not certain. We can only talk about the collection of possible outcomes. The collection of possible outcomes of an experiment is called the sample space and denoted by \\(Ω\\).\n\nExercise 4 What is the sample space for the toss of a coin?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Ω = \\{ H, T \\}\\).\n\n\n\n\nExercise 5 What is the sample space for the roll of a (6-sided) die?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Ω = \\{ 1,2,3,4,5,6 \\}\\).\n\n\n\nAn event is any subset of the sample space. If the outcome of the random experiment belongs to the event \\(A\\), we say that “the event \\(A\\) has occurred”. Some examples of events are:\n\nHead occurs in Exercise 4 (\\(A = \\{H\\}\\))\nBoth head and tail occur in Exercise 4 (\\(A = \\emptyset\\); this is an event that cannot happen, sometimes called the impossible event)\nAn even number is thrown in Exercise 5 (\\(A = \\{2,4,6\\}\\)).\n\nNote that events are subset of the sample space but not all subsets of a sample space may be events. The reasons are too complicated to explain, but the high-level explanation is that everything is okay for discrete sample spaces, but weird things can happen in continuous sample spaces.\nProbability (denoted by \\(\\PR\\)) is a function which assigns a number between \\(0\\) and \\(1\\) to every event. This number indicates what is the chance that the event occurs. Such a function should satisfy some axioms, which we will explain below.\nFirst, to define a function, we need to define it’s domain and range. Let’s denote the domain (i.e., the set of all events to which we can assign a probability) by \\(\\ALPHABET F\\). We expect probability to satisfy certain properties, which imposes constraints on the domain:\n\nProbability of an impossible event (e.g., getting both heads and tails when we toss a coin) should be zero. Thus, \\(\\emptyset \\in \\ALPHABET F\\).\nProbability of something happening (e.g., getting either a head or a tail when we toss a coin) should be one. Thus, \\(Ω \\in \\ALPHABET F\\).\nIf we assign probability to an event \\(A\\) then we should be able to assign probability to “\\(A\\) does not occur” i.e., \\(A^c\\). Thus, if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nIf we can talk about probability of \\(A\\) and \\(B\\), then we should be able to talk about probability that either \\(A\\) or \\(B\\) occurs and both \\(A\\) and \\(B\\) occur. Thus, if \\(A, B \\in \\ALPHABET F\\), then \\(A \\cup B \\in \\ALPHABET F\\) and \\(A \\cap B \\in \\ALPHABET F\\).\n\nThus, the domain of \\(\\PR\\) must be an algebra! However, when we go beyond finite sample spaces, being an algebra is not sufficient. But we first provide some examples of probability for finite sample spaces.\n\nExample 2 Consider a six-sided die where \\(Ω = \\{1, 2, \\dots, 6 \\}\\), \\(\\ALPHABET F = 2^Ω\\) and \\(\\PR\\) is given by \\[\n\\PR(A) = \\sum_{ω \\in A} q(ω),\n\\quad \\forall A \\in \\ALPHABET F\n\\] where \\[\nq(1) = q(2) = q(3) = q(4) = q(5) = \\frac 2{15}\n\\quad\\text{and}\\quad\nq(6) = \\frac{1}{3}.\n\\]\nVerify that\n\n\\(\\PR(Ω) = 1\\)\n\\(\\PR(\\{3,4,5\\}) = \\frac{6}{15}\\).\n\\(\\PR(\\{1,3,4,5\\}) = \\frac{8}{15}\\).\n\n\nWe now come back to the fact that restricting the domain of \\(\\PR\\) to be an algebra is not sufficient as is illustrated by the following example.\n\nExample 3 A coin is tossed repeatedly until a head turns up. The sample space is \\(Ω = \\{ω_1, ω_2, \\dots\\}\\) where \\(ω_n\\) denotes the event that the first \\(n-1\\) tosses are tails followed by a head.\n\nSuppose we are interested in finding the probability of the event that the coin is tossed an even number of times, i.e., \\(A = \\{ω_2, ω_4, \\dots\\}\\). Note that \\(ω_2, ω_4, \\dots \\in \\ALPHABET F\\). However, \\(A\\) is a set. If we want to assign probability to \\(A\\) in terms of probability of \\(ω_n\\), we require \\(\\ALPHABET F\\) to be closed under countable unions. This motivates the following definition.\n\n\n\n\n\n\n\\(σ\\)-algebra\n\n\n\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots\\}\\) of subsets of \\(Ω\\) is called a \\(\\boldsymbol{σ}\\)-algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under countable unions: if \\(A_1, A_2, \\dots \\in \\ALPHABET F\\), then \\[\n\\bigcup_{n=1}^∞ A_n \\in \\ALPHABET F\n\\]\n\n\n\n\n\n\n\n\n\nThe distinction between algebras and \\(σ\\)-algebras is technical. The reason that we need to consider \\(σ\\)-algebras is to do with the definition of probability on continuous sample spaces. Take \\(Ω = [0,1]\\) and consider a random experiment where “any outcome is equally likely”. Intuitively we capture this feature by assuming that for any interval \\([a,b]\\) with \\(0 \\le a \\le b \\le 1\\), we have \\[\\begin{equation}\\label{eq:uniform}\n\\PR([a,b]) = b - a.\n\\end{equation}\\]\nWe have seen that if we want \\(\\PR\\) to be a meaningful measure, the domain \\(\\ALPHABET F\\) must at least be an algebra. We have also seen that the power-set \\(2^Ω\\) is always an algebra. So, it is tempting to take \\(\\ALPHABET F = 2^{[0,1]}\\). However, it turns out that \\(2^{[0,1]}\\) has includes some weird sets (technically, non-measurable sets) due to which we cannot define a function \\(\\PR\\) on \\(2^{[0,1]}\\) that satisfies \\(\\eqref{eq:uniform}\\).\nTo workaround this technical limitation, we revisit the minimum requirements that we need from the domain of \\(\\PR\\). Since we are interested in \\(\\PR([a,b])\\), \\(\\ALPHABET F\\) must contains intervals (and therefore all finite unions and intersections of intervals). Since, we are working with continuous sample spaces, we also want \\(\\PR\\) to be continuous, i.e., for any sequence of sets \\(\\{A_n\\}_{n \\ge 1}\\), we want \\(\\PR(\\lim_{n \\to ∞} A_n) = \\lim_{n \\to ∞} \\PR(A_n)\\). It turns out that the additional requirement of continuity implies that \\(\\ALPHABET F\\) must be closed under countable unions as well. Thus, the domain \\(\\ALPHABET F\\) must at least be a \\(σ\\)-algebra.\nSo, we restrict to the simplest choice of the domain \\(\\ALPHABET F\\) needed for \\(\\eqref{eq:uniform}\\) and continuity to hold. For technical reasons, we need another property known as completeness. See Sec. 1.6 of the textbook.\n\n\n\n\n\n\n\n\n\n\\(σ\\)-algebra generated by a collection and Borel \\(σ\\)-algebra\n\n\n\nGiven collection \\(\\ALPHABET S\\) of subsets of \\(Ω\\), we have the following:\n\nThe power-set \\(2^Ω\\) contains \\(\\ALPHABET S\\). Therefore, there is at least one \\(σ\\)-algebra containing \\(\\ALPHABET S\\).\nIf \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are \\(σ\\)-algebras containing \\(\\ALPHABET S\\), then \\(\\ALPHABET F_1 \\cap \\ALPHABET F_2\\) also contains \\(\\ALPHABET S\\).\n\nThus, if we take the intersection of all \\(σ\\)-algebras containing \\(\\ALPHABET S\\), we get the smallest \\(σ\\)-algebra containing \\(\\ALPHABET S\\), which is sometimes denoted by \\(σ(\\ALPHABET S)\\).\nOne commonly used \\(σ\\)-algebra is the Borel \\(σ\\)-algebra, which is defined as follows.1 Let \\(Ω\\) be a subset of \\(\\reals\\) and \\(\\ALPHABET S\\) be the collection of all open intervals in \\(Ω\\). Then \\(σ(\\ALPHABET S)\\) is called the “Borel \\(σ\\)-algebra on Ω” and often denoted by \\(\\mathscr{B}(Ω)\\).\n\n\n1 Borel \\(σ\\)-algebra is usually defined for any topological space. We restrict our definition to subsets of reals.\nDefinition 1 (Probability space) A probability space is a tuple \\((Ω, \\ALPHABET F, \\PR)\\) comprising of a set \\(Ω\\), a \\(σ\\)-algebra \\(\\ALPHABET F\\) on \\(Ω\\), and a function \\(\\PR \\colon \\ALPHABET F \\to [0,1]\\) that satisfies the following axioms of proability\n\nNon-negativity. \\(\\PR(A) \\ge 0\\).\nNormalization. \\(\\PR(Ω) = 1\\).\nCountable additivity. If \\(A_1, A_2, \\dots Ω\\) is a collection of disjoint events in \\(\\ALPHABET F\\), then, \\[\n\\PR\\biggl( \\bigcup_{n=1}^∞ A_n \\biggr) =\n\\sum_{n=1}^∞ \\PR(A_n).\n\\]\n\n\nSome immediate implications of the axioms of probability are the following.\n\nLemma 1 (Properties of probability measures)  \n\nProbability of complement. \\(\\PR(A^c) = 1 - \\PR(A)\\).\nMonotonicity. If \\(A \\subset B\\), then \\(\\PR(B) = \\PR(A) + \\PR(B \\setminus A) \\ge \\PR(A)\\).\nInclusion-exclusion. Given two events \\(A\\) and \\(B\\), \\[\n\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B).\n\\]\nContinuity. Let \\(A_1, A_2, \\dots\\) be (weakly) increasing sequence of events, i.e., \\(A_1 \\subseteq A_2 \\subseteq A_3 \\subseteq \\cdots\\). Define \\[\n  A = \\lim_{n \\to ∞} A_n = \\bigcup_{n=1}^∞ A_n.\n\\] Then, \\[\n  \\PR(A) = \\lim_{n \\to ∞} \\PR(A_n).\n\\]\nSimilarly, let \\(B_1, B_2, \\dots\\) be (weakly) decreasing sequence of events, i.e., \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\cdots\\). Define \\[\n   B = \\lim_{n \\to ∞} B_n = \\bigcup_{n=1}^∞ B_n.\n\\] Then, \\[\n   \\PR(B) = \\lim_{n \\to ∞} \\PR(B_n).\n\\]\nUnion bound. For any sequence of events \\(\\{A_n\\}_{n \\ge 1}\\), we have \\[\n\\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr) \\le\n\\sum_{n=1}^{∞} \\PR(A_n).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof of parts (a)–(c) is elementary and left as an exercise. Part (d) is more technical and is essentially equivalent to countable additivity. See the textbook for a proof. Union bound is an immediate consequence of inclusion-exclusion and continuity.\n\n\n\n\nExample 4 Let \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B[0,1]\\), and \\(\\PR\\) be any probability measure on \\((Ω, \\ALPHABET F)\\). Take any \\(a \\in (0,1)\\).\n\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr)\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr]\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr)\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr]\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\n\nIn these examples, continuity implies that \\(\\PR(A) = \\lim_{n \\to ∞} \\PR(A_n)\\) and \\(\\PR(B) = \\lim_{n \\to ∞} \\PR(B_n)\\).\n\n\n\n\n\n\n\nSome terminology\n\n\n\n\nAn event \\(A\\) is called null if \\(\\PR(A) = 0\\). Null event should not be confused with impossible event \\(\\emptyset\\).\nWe say that \\(A\\) occurs almost surely (abbreviated to a.s.) if \\(\\PR(A) = 1\\).\n\n\n\n\nExample 5 Consider \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B([0,1])\\), and \\(\\PR\\) to be the uniform probability distribution on \\(Ω\\). Consider the event \\(A\\) that the outcome is a rational number. \\(A\\) is a countable set (because the set of rational numbers is countable). For any \\(x \\in A\\), \\(\\{x\\} \\in \\ALPHABET F\\), and \\(\\PR(\\{x\\}) = 0\\) (we can infer this from Example 4 by thinking of \\(\\{x\\}\\) as the limit of intervals \\(\\bigl[x, x+ \\frac 1n\\bigr]\\)). Thus, by countable additivity, \\(\\PR(A) = 0\\). Hence, \\(A\\) is null.\nThe above analysis implies that \\(\\PR(A^c) = 1\\), thus the event that the outcome is irrational occurs almost surely."
  },
  {
    "objectID": "probability-spaces.html#conditional-probability",
    "href": "probability-spaces.html#conditional-probability",
    "title": "Introduction to Probability",
    "section": "4 Conditional Probability",
    "text": "4 Conditional Probability\nConditional probabilities quantify the uncertainty of an event when it is known that another event has occurred\n\nDefinition 2 Let \\((Ω,\\ALPHABET F, \\PR)\\) be a probability space and \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\). Then, the conditional probability that \\(A\\) occurs given that \\(B\\) occurs is defined as \\[\n\\PR(A | B) = \\dfrac{ \\PR(A \\cap B) }{ \\PR(B) }.\n\\]\n\nThe notation \\(\\PR(A | B)\\) is read as “probability of \\(A\\) given \\(B\\)” or “probability of \\(A\\) conditioned on \\(B\\)”.\n\nExercise 6 Suppose we roll a fair six-sided die (a fair die means that all outcomes are equally likely). Consider the events \\(A\\) that the outcomes is prime and \\(B\\) that the outcome is a multiple of \\(3\\). Compute \\(\\PR(A | B)\\) and \\(\\PR(B | A)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(Ω = \\{1, 2, 3, 4, 5, 6\\}\\), \\(A = \\{2, 3, 5\\}\\), and \\(B = \\{3, 6\\}\\). Note that \\(\\PR(A) = \\frac 12\\) and \\(\\PR(B) = \\frac 13\\).\nThus, \\[ \\PR(A | B) = \\frac{ \\PR(A \\cap B) }{ \\PR(B) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{3,6\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{3,6\\}} } = \\frac {1}{2}.\n\\] Similarly, \\[ \\PR(B | A) = \\frac{ \\PR(B \\cap A) }{ \\PR(A) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{2,3,5\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{2,3,5\\}} } = \\frac {1}{3}.\n\\]\n\n\n\n\nExercise 7 Suppose we roll two fair six-sided dice. Consider the event \\(A\\) that the minimum of the two rolls is greater than or equal to \\(6\\) and the event \\(B\\) that the maximum of the two rolls is less than or equal to \\(8\\). Compute \\(\\PR(A|B)\\) and \\(\\PR(B|A)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\(Ω = \\{ 1, 2,3, 4, 5, 6\\}^2\\) and \\(\\PR\\) is uniform probability on all outcomes. The sets \\(A\\), \\(B\\), and \\(A \\cap B\\) are shown in Figure 2. Note that \\(\\PR(A) = \\PR(B) = \\frac{26}{36} = \\frac{13}{18}\\).\n\n\n\n\n\n\nFigure 2: The different events in Exercise 7\n\n\n\nThus, we have \\[\n\\PR(A|B) = \\frac{ \\PR(A \\cap B) } { \\PR(B) }\n= \\frac{ \\ABS{ A \\cap B} }{ \\ABS{B} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\] and \\[\n\\PR(B|A) = \\frac{ \\PR(B \\cap A) } { \\PR(A) }\n= \\frac{ \\ABS{ B \\cap A} }{ \\ABS{A} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\]\n\n\n\n\n\n\n\n\n\nConditional probabilities are probabilities\n\n\n\nConditional probabilities are legitimate probability measures on \\((Ω, \\ALPHABET F)\\). In particular, fix event \\(B\\) with \\(\\PR(B) &gt; 0\\). Then\n\n\\(\\PR(A \\mid B) \\ge 0\\).\n\\(\\PR(Ω \\mid B) = \\dfrac{\\PR(Ω \\cap B)}{\\PR(B)} = 1\\).\nFor disjoint events \\(A_1, A_2 \\in \\ALPHABET F\\), \\[\\PR(A_1 \\cup A_2 \\mid B) =\n\\frac{ \\PR( (A_1 \\cup A_2) \\cap B) }{ \\PR(B) } =\n\\frac{ \\PR( (A_1 \\cap B) \\cup (A_2 \\cap B) ) }{ \\PR(B) } =\n\\frac{ \\PR(A_1 \\cap B) + \\PR (A_2 \\cap B) }{ \\PR(B) } =\n\\PR(A_1 \\mid B) + \\PR(A_2 \\mid B)\\] where we have used the fact that \\((A_1 \\cap B)\\) and \\((A_2 \\cap B)\\) are disjoint.\n\n\n\n\nExercise 8 Given an event \\(B\\) with \\(\\PR(B) &gt; 0\\), show that\n\n\\(\\PR(A^c | B) = 1 - P(A|B)\\).\n\\(\\PR(A_1 \\cup A_2 | B) = \\PR(A_1 | B) + \\PR(A_2 | B) - \\PR(A_1 \\cap A_2 | B)\\).\nIf \\(A_1 \\subset A_2\\) then \\(\\PR(A_1 | B) \\le \\PR(A_2 | B)\\).\n\n\nThe definition of conditional probability gives rise to the chain rule.\n\nLemma 2 (Chain rule of probability) Let \\(A\\) and \\(B\\) be events in a probability space \\((Ω, \\ALPHABET F, \\PR)\\).\n\nIf \\(\\PR(B) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(A | B) \\PR(B)\\).\nIf \\(\\PR(A) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(B | A) \\PR(A)\\).\n\n\nCombining the chain rule with the basic properties of partitions, we get the law of total probability.\n\nLemma 3 (Law of total probability) Let \\(\\{B_1, B_2, \\dots, B_m\\}\\) be a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(A) = \\sum_{i=1}^m \\PR(A \\cap B_i)\n= \\sum_{i=1}^m \\PR(A | B_i) \\PR(B_i).\n\\]\n\n\n\n\n\n\n\n\n\nFigure 3: Illustration of Law of total probability\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(m=2\\), in which case the result can be simplified as \\[\\PR(A) = \\PR(A|B)\\PR(B) + \\PR(A|B^c) \\PR(B^c).\\]\nTo prove this observe that \\[\\begin{equation}\\label{eq:two-step}\nA = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c).\n\\end{equation}\\] The events \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint. Therefore, by additivity, we have \\[\n\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c).\n\\] Then, by the definition of conditional probability, we have \\(\\PR(A \\cap B) = \\PR(A|B) \\PR(B)\\) and \\(\\PR(A \\cap B^c) = \\PR(A|B^c) \\PR(B^c)\\). Substituting in the above, we get \\(\\eqref{eq:two-step}\\).\nThe argument for the general case is similar.\n\n\n\n\nExercise 9 There are two routes for a packet to be transmitted from a source to the destination.\n\nThe packet takes route \\(R_1\\) with probability \\(\\frac 34\\) and takes route \\(R_2\\) with probability \\(\\frac 14\\).\nOn route \\(R_1\\), the packet is dropped with probability \\(\\frac 13\\).\nOn route \\(R_2\\), the packet is dropped with probability \\(\\frac 14\\).\n\nFind the probability that the packet reaches the destination.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe start by defining some events: Let \\(R_1\\) denote the event that the packet took route \\(R_1\\) and \\(R_2\\) denote the event that the packet took route \\(R_2\\). Let \\(D\\) denote the event that the packet was dropped.\nThen, the information given in the question can be written as:\n\n\\(\\PR(R_1) = \\frac 34\\) and \\(\\PR(R_2) = \\frac 14\\).\n\\(\\PR(D | R_1) = \\frac 13\\). Thus, \\(\\PR(D^c | R_1) = 1 - \\PR(D | R_1) = \\frac 23\\).\n\\(\\PR(D | R_2) = \\frac 14\\). Thus, \\(\\PR(D^c | R_2) = 1 - \\PR(D | R_2) = \\frac 34\\).\n\nThen, by the law of total probability, we have \\[\\begin{align*}\n\\PR(D^c) &= \\PR(D^c | R_1) \\PR(R_1) + \\PR(D^c | R_2) \\PR(R_2) \\\\\n&= \\frac 34 \\frac 23 + \\frac 14 \\frac 34\n= \\frac {11}{16}.\n\\end{align*}\\]\n\n\n\n\nLemma 4 (Bayes rule) For any events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(A), \\PR(B) &gt; 0\\), we have \\[\n\\PR(B|A) = \\dfrac{\\PR(A|B)\\PR(B)}{\\PR(A)}.\n\\]\nIn general, if \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(B_i|A) =\n\\dfrac{ \\PR(A|B_i) \\PR(B_i) }\n{\\displaystyle \\sum_{j=1}^m \\PR(A|B_j) \\PR(B_j)}\n\\] where we have used the law of total probability (Lemma 3) in the denominator.\n\n\nExercise 10 Consider the model of Exercise 9. Suppose we know that the packet was dropped. What is the probability that it was transmitted via route \\(R_1\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall the events \\(R_1\\), \\(R_2\\), and \\(D\\) defined in the solution of Exercise 9. We were given that \\[\\PR(R_1) = \\frac 34, \\quad \\PR(R_2) = \\frac 14, \\quad\n\\PR(D|R_1) = \\frac 13, \\quad \\PR(D|R_2) = \\frac 14.\\] We had compute that \\[\n\\PR(D) = 1 - \\PR(D^c) = \\frac 5{16}.\n\\]\nThus, by Bayes rule, we have \\[\n\\PR(R_1 | D) = \\frac{ \\PR(D | R_1) \\PR(R_1) }{ \\PR(D) }\n= \\frac{ \\frac 13 \\frac 34 } { \\frac 5{16} } = \\frac 45.\n\\]"
  },
  {
    "objectID": "probability-spaces.html#independence",
    "href": "probability-spaces.html#independence",
    "title": "Introduction to Probability",
    "section": "5 Independence",
    "text": "5 Independence\nIn general, the knowledge that an event \\(B\\) has occurred changes the probability of event \\(A\\), since \\(\\PR(A)\\) is replaced by \\(\\PR(A|B)\\). If the knowledge that \\(B\\) has occurred does not does not change our belief about \\(A\\), i.e., when \\(\\PR(A|B) = \\PR(A)\\), we say “\\(A\\) and \\(B\\) are independent”. This leads to the following definition.\n\nDefinition 3 The events \\(A, B \\in \\ALPHABET F\\) are called independent if \\[\n\\PR(A|B) = \\PR(A)\n\\quad\\text{or}\\quad\n\\PR(B|A) = \\PR(B).\n\\] An alternative but equivalent definition is \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B).\n\\]\nWe will use the notation \\(A \\independent B\\) to denote that the events \\(A\\) and \\(B\\) are independent.\n\n\n\n\n\n\n\nIt is common for students to make the mistake and think that independence means \\(A \\cap B = \\emptyset\\). This is not true!\n\n\n\n\nExample 6 The events \\(A\\) and \\(B\\) defined in Exercise 6 are independent.\n\n\nExample 7 The events \\(A\\) and \\(B\\) defined in Exercise 7 are not independent.\n\n\nExercise 11 \\(A \\independent B\\) implies the following:\n\n\\(A \\independent B^c\\).\n\\(A^c \\independent B\\).\n\\(A^c \\independent B^c\\).\n\n\n\nDefinition 4 A family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is called independent (sometimes mutually independent if for all non-empty subset of indices \\(\\{k_1, \\dots, k_m\\} \\subset \\{1,\\dots,n\\}\\), we have \\[\n\\PR\\bigl( A_{k_1} \\cap A_{k_2} \\cap \\cdots \\cap A_{k_m} \\bigr)\n= \\PR(A_{k_1}) \\PR(A_{k_2}) \\cdots \\PR(A_{k_m}).\n\\]\n\n\nExercise 12 Three bits are transmitted over a noisy channel. For each bit, the probability of correct reception is \\(λ\\). The error events for the three transmissions are mutually independent. Find the probability that two bits are received correctly.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(i \\in \\{1, 2, 3\\}\\), let\n\n\\(E_i\\) denote the event that bit \\(i\\) is received incorrectly\n\\(C_i\\) denote the event that bit \\(i\\) is received correctly\n\nMoreover let \\(S\\) denote the event that two bits are received correctly. Then, \\[\nS = (C_1 \\cap C_2 \\cap E_3) \\cup (C_1 \\cap E_2 \\cap C_3)\n\\cap (E_1 \\cap C_2 \\cap C_3).\n\\] Note that the three events in the right hand side are disjoint. Thus, \\[\\begin{align*}\n\\PR(S) &=\n\\PR(C_1 \\cap C_2 \\cap E_3) +  \\PR(C_1 \\cap E_2 \\cap C_3) +\n\\PR(E_1 \\cap C_2 \\cap C_3) \\\\\n&=\n\\PR(C_1)\\PR(C_2)\\PR(E_3) +  \\PR(C_1)\\PR(E_2)\\PR(C_3) +\n\\PR(E_1)\\PR(C_2)\\PR(C_3) \\\\\n&= 3 (1-λ) λ^2.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nPairwise independence vs independence\n\n\n\nA family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is pairwise independent if for every \\(i,j \\in \\{1, \\dots, n\\}\\), \\(i \\neq j\\), we have \\[ \\PR(A_i \\cap A_j) = \\PR(A_i) \\PR(A_j). \\]\nPairwise independence is weaker than Independence. For instance, three events \\(A\\), \\(B\\), and \\(C\\) are pairwise independent if \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B),\n\\quad\n\\PR(B \\cap C) = \\PR(B) \\PR(C),\n\\quad\\text{and}\\quad\n\\PR(C \\cap A) = \\PR(C) \\PR(A).\n\\] For independence, in addition to the above, we also need \\[ \\PR(A \\cap B \\cap C) = \\PR(A)\\PR(B) \\PR(C). \\]\nThe following example illustrates shows that independence is stronger than pairwise independence. Consider an urn with \\(M\\) red balls and \\(M\\) blue balls. Two balls are drawn at random, one at a time, with replacement. Consider the following events:\n\n\\(A\\) is the event that the first ball is red.\n\\(B\\) is the event that the second ball is blue.\n\\(C\\) is the event that both balls are of the same color.\n\nObserve that\n\n\\(A \\cap B\\) is the event that the first is red and second is blue.\n\\(B \\cap C\\) is the event that both balls are blue.\n\\(C \\cap A\\) is the event that both balls are red.\n\\(A \\cap B \\cap C = \\emptyset\\)\n\nTherefore,\n\n\\(\\PR(A) = \\PR(B) = \\PR(C) = \\frac 14\\).\n\\(\\PR(A \\cap B) = \\PR(B \\cap C) = \\PR(C \\cap A) = \\frac 14\\).\n\\(\\PR(A \\cap B \\cap C) = \\emptyset\\).\n\nThus, \\(A\\), \\(B\\), \\(C\\) are pairwise independent but not independent."
  }
]