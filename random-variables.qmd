---
title: Random variables and random vectors
---

In many situations, we are not directly interested in the outcome of a random experiment, but a consequence of the outcome. Such consequences may be thought of as a function of the outcome. When they are real-valued, such functions of the outcome are called **random variables**.

:::{#exm-coin-toss}
Suppose a fair coin is tossed twice. Thus, $Ω = \{ HH, HT, TH, TT \}$, $\ALPHABET F = 2^Ω$, and $\PR$ is such that all outcomes are equally likely. For an $ω \in Ω$, let $X(ω)$ denote the number of heads. Thus,
$$
X(HH) = 2, \quad X(HT) = 1, \quad X(TH) = 1, \quad X(TT) = 0.
$$
:::

The random variable $X \colon Ω \to \reals$ induces a probability measure on $\reals$. Formally, to define such a probability measure, we need an associated $σ$-algebra on $\reals$. As discussed in [last lecture](probability-spaces.qmd#Borel-algebra), the commonly used $σ$-algebra on reals is the Borel $σ$-algebra, $\mathscr{B}(\reals)$. For everything to be consistent, we require the function $X$ to satisfy a property known as **measurability**.

:::{#def-measurability}
A **random variable** is a function $X \colon Ω \to \reals$ with the property that for every $x \in \reals$ the event $A(x) \coloneqq \{ω \in Ω : X(ω) \le x \} \in \ALPHABET F$. Such functions are said to be **$\ALPHABET F/\mathscr{B}(\reals)$-measurable**.

The **cumulative distribution function (CDF)** of a random variable $X$ is the function $F \colon \reals \to [0,1]$ given by
$$
F(x) \coloneqq \PR(A(x)) = \PR(\{ ω \in Ω : X(ω) \le x \}).
$$
:::

:::{.callout-note}
### Some comments on notation

1. The standard notation in probability theory is to use uppercase letters such as $X$, $Y$, $Z$, etc. to denote random variables and the corresponding lowercase letters $x$, $y$, $z$, etc. to denote the possible numerical values of these variables. 

2. Events such as $\{ω \in Ω : X(ω) \le x \}$ are commonly abbreviated as $\{ω : X(ω) \le x \}$ or $\{X \le x\}$. Thus, we have
   
   - $\PR(X \le x) = \PR(\{ω \in Ω : X(ω) \le x \})$.
   - $\PR(X = x) = \PR(\{ω \in Ω : X(ω) = x \})$.
   - $\PR(x < X \le y) = \PR(\{ω \in Ω : x < X(ω) \le y \})$.
   - For any (Borel) subset $B$ of $\reals$, 
     $\PR(X \in B) = \PR(\{ω \in Ω : X(ω) \in B \})$.

3. When we need to emphasize the dependence of the CDF on the random variable, we use the notation $F_X$, etc. 
:::

For instance, for @exm-coin-toss, the CDF is given by 
$$
F_X(x) = \begin{cases}
0, & \hbox{if } x < 0,  \\
\frac 14, & \hbox{if } 0 \le x < 1, \\
\tfrac 34, & \hbox{if } 1 \le x < 2,\\
1, &\hbox{if } 2 \le x. 
\end{cases}$$

:::{#lem-properties-of-CDFs}
CDFs have the following properties.

1. $\lim_{x \to -∞} F(x) = 0$ and $\lim_{x \to +∞} F(x) = 1$.

2. CDFs are non-decreasing, i.e., if $x < y$, then $F(x) \le F(y)$.

3. CDFs are right continuous, i.e., $\lim_{h \downarrow 0}F(x+h) = F(x)$.
:::

:::{#exm-constant}
### Constant random variables

The simplest random variable takes a constant value on the whole domain $Ω$, i.e., 
$$
X(ω) = c, \quad \forall ω \in Ω
$$
where $c$ is a constant. The CDF $F(x) = \PR(X \le x)$ is the step function
$$
F(x) = \begin{cases}
0, & x < c \\
1, & x \ge c.
\end{cases}
$$

Slightly more generally, we say that $X$ is _almost surely_ a constant if there exists a $c \in \reals$ such that $\PR(X=c) = 1$.
:::

:::{#exm-bernoulli}
### Bernoulli random variable
A Bernoulli random variable takes two possible values: value $0$ with probability $1-p$ and value $1$ with probability $p$. It's CDF is given by
$$
F(x) = \begin{cases}
0, & x < 0 \\
1 - p, & 0 \le x < 1 \\
1, & x \ge 1.
\end{cases}
$$
:::

:::{#exm-indicator-functions}
### Indicator functions
Let $A$ be an event. Define the _indicator of event $A$_ $\IND_{A} \colon Ω \to \reals$ as 
$$ \IND_{A}(ω) = \begin{cases}
    1, & \hbox{if } ω \in A \\
    0, & \hbox{otherwise }
\end{cases}.$$
Observe that $\IND_A$ is a Bernoulli random variable which takes values $1$ and $0$ with probabilities $\PR(A)$ and $1 - \PR(A)$. 
:::

:::{#lem-more-properties-of-CDFs}
Let $F$ be the distribution function of $X$. Then,

1. $\PR(X > x) = 1 - F(x)$. 

2. $\PR(x < X \le y) = F(y) - F(x)$.

3. $\PR(X = x) = F(x) - F(x^{-})$.
:::

## Discrete and coninuous random variables

In the engineering literature, it is common to encouter two special type of random variables: discrete and continuous. 

1. A random variable $X$ is said to be **discrete** if it takes values in a countable subset $\{x_1, x_2, \dots\}$ of $\reals$. A discrete random variable has a **probability mass function (PMF)** $f \colon \reals \to [0,1]$ which satisfies the following properties:

    - $f(x) = \PR(X = x) = F(x) - F(x^{-1})$.
    - $F(x) = \sum_{x_i : x_i \le x} f(x_i).$
    - An implication of the above is $\sum_{i=1}^{∞} f(x_i) = 1$. 


2. A random variable $X$ is called **continuous** if there exists an integrable function $f \colon \reals \to [0, ∞)$ called the **probability denisity function** such that the CDF can be written as
    $$
    F(x) = \int_{-∞}^x f(x) dx.
    $$


Note that random variables are not restricted to being discrete or continuous. For instance, consider the following random experiment. A fair coin is tossed: if the outcome is heads, then $X \sim \text{Bernoulli}(0.25)$; if the outcome is tails; then $X \sim \text{Uniform}[0,1]$. Thus (from the law of total probability), the CDF of $X$ is
$$
F_X(x) = \begin{cases}
0, & \hbox{if } x < 0 \\
\frac 12 & \hbox{if } x = 0 \\
\frac {(1+x)}2 & \hbox{if } 0 < x < 1 \\
1 & \hbox{if } x \ge 1.
\end{cases}.
$$

Note that discrete random variable creates a parition of the sample space. In particular, suppose $X$ is a random variable and $\{x_1, x_2, \dots\}$ is the range of $X$. Define
$$A_i = \{ω \in Ω : X(ω) = x_i \} = X^{-1}(x_i)$$
Then, $\{A_1, A_2, \dots \}$ is a partition of $Ω$. 

:::{.callout-note collapse="true"}
### Proof of claim

To show that $\{A_1, A_2, \dots \}$ forms a partition, we need to establish two properties:

1. $A_i \cap A_j = \emptyset$.

2. $\bigcup_{i=1}^∞ A_i = Ω$.

The details are left as an exercise.
:::

The power-set of $\{A_1, A_2, \dots\}$ is called the **$σ$-algebra generated by $X$** and denoted by $σ(X)$. This $σ$-algebra captures the crux of measurability. As an illustration, let's reconsider @exm-coin-toss. In this case, the range of $X$ is $\{0, 1, 2\}$. The partition corresponding to $σ(X)$ is shown in @fig-sigma-X.

![Illustration of $σ(X)$ for @exm-coin-toss](TODO){#fig-sigma-X}


## Random vectors and joint distributions.


